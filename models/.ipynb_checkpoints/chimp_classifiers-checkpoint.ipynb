{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kaggle package could not be loaded!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeptone.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_predictions(x):\n",
    "    # Use the sampled (at 16kHz) amplitude time-series of the sound file as input in DeepTone models\n",
    "    # to obtain the embedding representations of the latent space for each sample \n",
    "    identity = deeptone.models.Identity.predict(x)[0]\n",
    "    # Create time index for each embedding vector respective to deeptone's output of 64 ms per \n",
    "    ts_index = pd.TimedeltaIndex(data=np.cumsum(np.ones((identity.shape[0],))*64), unit='ms')\n",
    "    # Save each vector embedding as coloumn, indexed (row-wise) by the time signature of each vector embedding elements\n",
    "    df_ = pd.DataFrame(index = ts_index)\n",
    "    for i in range(identity.shape[1]):\n",
    "        df_[f'identity_{i}'] = identity[:,i]\n",
    "    return df_\n",
    "\n",
    "def embedding_processing(dfi_keep,dfs,dfs_norm): \n",
    "    # Calculates embeddings and averages over time\n",
    "    audio_files = list(dfi_keep.path_to_file.values)\n",
    "    embeddings = np.ndarray((len(audio_files), 128))\n",
    "    embeddings_norm = np.ndarray((len(audio_files), 128))\n",
    "    for idx, audio_file in enumerate(audio_files):\n",
    "        df_ = dfs[audio_file]\n",
    "        df_norm = dfs_norm[audio_file]\n",
    "        embeddings[idx,:] = df_.mean(axis=0) \n",
    "        embeddings_norm[idx,:] = df_norm.mean(axis=0)\n",
    "    return embeddings, embeddings_norm\n",
    "\n",
    "def plot_count_variable_combo(target,nested,dfi_keep):\n",
    "    fig_dims = (15, 10)\n",
    "    fig, ax = plt.subplots(figsize=fig_dims)\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    g=sns.histplot(dfi_keep, x=target, hue=nested, multiple=\"stack\",ax=ax).set(xlabel=None)\n",
    "    plt.xticks(rotation=90,size=20)\n",
    "    plt.yticks(rotation=90,size=20)\n",
    "    plt.ylabel(\"Count\",size=20)\n",
    "    plt.subplots_adjust(top=0.85,bottom=0.2)\n",
    "    plt.legend(labels=[\"Pant-hoot-intro\",\"Combinations\",\"Pant-hoot-climax\",\"Scream\",\"Pant-grunt\"])\n",
    "#     plt.legend(labels=[\"Combinations\",\"Pant-hoot-intro\",\"Pant-hoot-climax\",\"Scream\"],title=\"Call types\")\n",
    "    plt.setp(ax.get_legend().get_texts(), fontsize='15') # for legend text\n",
    "    plt.setp(ax.get_legend().get_title(), fontsize='20') # for legend title\n",
    "#     plt.set_xlabel(fontsize=30)\n",
    "    fig.savefig(f'Chimps_call_type_X_ID.png') \n",
    "    \n",
    "# plot_count_variable_combo(\"ID\",\"quality\")\n",
    "\n",
    "def Cluster_on_PCA(data,true_label_names,n_comps,cluster_options,vers,cluster_type):\n",
    "    # data = embeddings\n",
    "    # true_label_names = dfi_keep.loc[:,\"ID\"].values\n",
    "    label_encoder = LabelEncoder()\n",
    "    true_labels = label_encoder.fit_transform(true_label_names)\n",
    "    n_clusters = len(label_encoder.classes_)\n",
    "    # n_comps = 3\n",
    "    preprocessor = Pipeline([ (\"scaler\", MinMaxScaler()), (\"pca\", PCA(n_components=n_comps)), ])\n",
    "    if cluster_type == \"kmeans\":\n",
    "        clusterer = Pipeline([(cluster_type,KMeans(n_clusters=n_clusters,init=cluster_options,n_init=50, max_iter=500,), ), ])\n",
    "    elif cluster_type == \"spectral\":\n",
    "        clusterer = Pipeline([(cluster_type,SpectralClustering(n_clusters=n_clusters,affinity=cluster_options,), ), ])\n",
    "    else:\n",
    "        EnvironmentError(f'{cluster_type} not a supported cluster')\n",
    "        \n",
    "    pipe = Pipeline(\n",
    "        [\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"clusterer\", clusterer)\n",
    "        ]\n",
    "    )\n",
    "    pipe.fit(data)\n",
    "    preprocessed_data = pipe[\"preprocessor\"].transform(data)\n",
    "    predicted_labels = pipe[\"clusterer\"][cluster_type].labels_\n",
    "    silhouette_score(preprocessed_data, predicted_labels)\n",
    "    adjusted_rand_score(true_labels, predicted_labels)\n",
    "    pcadf = pd.DataFrame(\n",
    "        pipe[\"preprocessor\"].transform(data),\n",
    "        columns=[\"component_\"+str(f+1) for f in list(range(n_comps))]\n",
    "    )\n",
    "    pcadf[\"predicted_cluster\"] = pipe[\"clusterer\"][cluster_type].labels_\n",
    "    pcadf[\"true_label\"] = label_encoder.inverse_transform(true_labels)\n",
    "    fig_dims = (15, 10)\n",
    "    fig, ax = plt.subplots(figsize=fig_dims)\n",
    "    plt.style.use(\"fivethirtyeight\")\n",
    "    scat = sns.scatterplot(\n",
    "        \"component_1\",\n",
    "        \"component_2\",\n",
    "        s=50,\n",
    "        data=pcadf,\n",
    "        hue=\"predicted_cluster\",\n",
    "        style=\"true_label\",\n",
    "        palette=\"Set2\",ax=ax\n",
    "    )\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    scat.set_title(\n",
    "        f'Spectral clustering of PCA projection of Chimp individuals within \"Scream\" call types'\n",
    "    )\n",
    "    plt.subplots_adjust(top=0.85,bottom=0.2)\n",
    "    plt.savefig(f'../../derived_data/figures/Chimps/Chimps_KMeans_PCA_{slow_down}_{vers}_{normalise_embeddings}_{cluster_type}_{n_comps}.png')\n",
    "    \n",
    "def SVM(data,true_label_names,n_rand,standardize,seed,stochasticc):\n",
    "    # Peform randomised param search: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
    "    if standardize == True:\n",
    "        scaler = StandardScaler()\n",
    "        data = scaler.fit_transform(embeddings)\n",
    "    else:\n",
    "        data = embeddings\n",
    "    random.seed(seed)\n",
    "    states = random.sample(list(np.arange(1,1000)),n_rand)\n",
    "    scores = np.zeros(n_rand)\n",
    "    acc_scores = np.zeros(n_rand)\n",
    "    rec_scores = np.zeros(n_rand)\n",
    "    CC = np.zeros(n_rand)\n",
    "    degrees = np.zeros(n_rand)\n",
    "    row=-1\n",
    "    y_true_a = np.array([])\n",
    "    y_pred_a = np.array([])\n",
    "    for state in states:\n",
    "        row+=1\n",
    "        X_train, X_test, y_train, y_test = train_test_split(data, true_label_names, test_size=0.3,random_state=state)\n",
    "        # state = 130\n",
    "        regul = 2\n",
    "        deg = 10\n",
    "        if stochasticc == False:\n",
    "            # Grid search\n",
    "            parameters = {'kernel':['poly'], 'degree':[1, 20],'C':np.arange(1,10,1)}\n",
    "            svc = svm.SVC(kernel='poly', degree=deg, C=regul, decision_function_shape='ovo',gamma='scale',n_jobs=-1)\n",
    "            f1_macro = make_scorer(f1_score, average='macro')\n",
    "            clf = GridSearchCV(svc, parameters)#,scoring=f1_macro)\n",
    "            search = clf.fit(X_train, y_train)\n",
    "            poly = svm.SVC(kernel='poly', degree=search.best_params_['degree'], C=search.best_params_['C'], decision_function_shape='ovo',gamma='scale').fit(X_train, y_train)\n",
    "            poly_pred = poly.predict(X_test)\n",
    "        else:\n",
    "            # Stochastich search \n",
    "            distributions = dict(C=uniform(loc=1, scale=30), degree = list(range(20))) #list(range(20)) uniform(loc=1, scale=20)\n",
    "            poly = svm.SVC(kernel='poly', degree=deg, C=regul, decision_function_shape='ovo',gamma='scale')\n",
    "            clf = RandomizedSearchCV(poly, distributions, random_state=0,n_jobs=-1)\n",
    "            search = clf.fit(X_train, y_train)\n",
    "            search.best_params_\n",
    "            poly = svm.SVC(kernel='poly', degree=search.best_params_['degree'], C=search.best_params_['C'], decision_function_shape='ovo',gamma='scale').fit(X_train, y_train)\n",
    "            poly_pred = poly.predict(X_test)\n",
    "        scores[row]=f1_score(y_test, poly_pred,average=\"macro\")\n",
    "        degrees[row]=search.best_params_['degree']\n",
    "        acc_scores[row]=accuracy_score(y_test, poly_pred)\n",
    "        rec_scores[row]=recall_score(y_test, poly_pred,average=\"macro\")\n",
    "        CC[row]=search.best_params_['C']\n",
    "        y_true_a=np.append(y_true_a,y_test)\n",
    "        y_pred_a=np.append(y_pred_a,poly_pred)\n",
    "    dataa={\n",
    "        'f1_scores': scores,\n",
    "        'acc_scores': acc_scores,\n",
    "        'rec_scores': rec_scores,\n",
    "        'C': CC,\n",
    "        'degree': degrees \n",
    "    }\n",
    "    df_ = pd.DataFrame(data=dataa)\n",
    "    return df_, y_true_a, y_pred_a \n",
    "def RandomForrest(Xnorm_,y_,n_rand,seed):\n",
    "    n_reps = n_rand\n",
    "    scores = np.zeros(n_reps)\n",
    "    n_estimators = np.zeros(n_reps)\n",
    "    random.seed(seed)\n",
    "    states = random.sample(list(np.arange(1,1000)),n_rand)\n",
    "    scores = np.zeros(n_rand)\n",
    "    acc_scores = np.zeros(n_rand)\n",
    "    rec_scores = np.zeros(n_rand)\n",
    "    n_estimators = np.zeros(n_rand)\n",
    "    CC = np.zeros(n_rand)\n",
    "    degrees = np.zeros(n_rand)\n",
    "    rep_idx=-1\n",
    "    y_true_a = np.array([])\n",
    "    y_pred_a = np.array([])\n",
    "    for state in states:\n",
    "        rep_idx+=1\n",
    "        X_train, X_test, y_train, y_test = train_test_split(Xnorm_, y_, test_size = 0.25,random_state =state)#, random_state = random_state)\n",
    "        distributions = dict(n_estimators=list(range(100)))\n",
    "        rlf = RandomForestClassifier(n_estimators = 100,\n",
    "                                     class_weight='balanced_subsample',\n",
    "                                     bootstrap=True,\n",
    "                                     max_samples=0.5,\n",
    "                                     max_features=1.0)#, random_state = random_state)\n",
    "        clf = RandomizedSearchCV(rlf, distributions, random_state=0,n_jobs=-1)\n",
    "        search = clf.fit(X_train, y_train)\n",
    "        clf = RandomForestClassifier(n_estimators = search.best_params_['n_estimators'],\n",
    "                                     class_weight='balanced_subsample',\n",
    "                                     bootstrap=True,\n",
    "                                     max_samples=0.5,\n",
    "                                     max_features=1.0)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_pred_train = clf.predict(X_train)\n",
    "        scores[rep_idx]=f1_score(y_test, y_pred,average=\"macro\")\n",
    "        acc_scores[rep_idx]=accuracy_score(y_test, y_pred)\n",
    "        rec_scores[rep_idx]=recall_score(y_test, y_pred,average=\"macro\")\n",
    "        n_estimators[rep_idx]=search.best_params_['n_estimators']\n",
    "        y_true_a=np.append(y_true_a,y_test)\n",
    "        y_pred_a=np.append(y_pred_a,y_pred)\n",
    "    dataa={\n",
    "        'f1_scores': scores,\n",
    "        'acc_scores': acc_scores,\n",
    "        'rec_scores': rec_scores,\n",
    "        'n_estimaors': n_estimators\n",
    "    }\n",
    "    df_ = pd.DataFrame(data=dataa)\n",
    "    return df_, y_true_a, y_pred_a \n",
    "\n",
    "def GP(Xnorm_,y_,n_rand,seed):\n",
    "    n_reps = n_rand\n",
    "    scores = np.zeros(n_reps)\n",
    "    n_estimators = np.zeros(n_reps)\n",
    "    random.seed(seed)\n",
    "    states = random.sample(list(np.arange(1,1000)),n_rand)\n",
    "    scores = np.zeros(n_rand)\n",
    "    acc_scores = np.zeros(n_rand)\n",
    "    rec_scores = np.zeros(n_rand)\n",
    "    CC = np.zeros(n_rand)\n",
    "    degrees = np.zeros(n_rand)\n",
    "    rep_idx=-1\n",
    "    kernel = kerns.RationalQuadratic()\n",
    "    y_true_a = np.array([])\n",
    "    y_pred_a = np.array([])\n",
    "    for state in states:\n",
    "        rep_idx+=1\n",
    "        X_train, X_test, y_train, y_test = train_test_split(Xnorm_, y_, test_size = 0.25,random_state =state)#, random_state = random_state)\n",
    "        gpc = GaussianProcessClassifier(kernel=kernel,random_state=state).fit(X_train, y_train)\n",
    "        y_pred = gpc.predict(X_test)\n",
    "        y_pred_train = gpc.predict(X_train)\n",
    "        scores[rep_idx]=f1_score(y_test, y_pred,average=\"macro\")\n",
    "        acc_scores[rep_idx]=accuracy_score(y_test, y_pred)\n",
    "        rec_scores[rep_idx]=recall_score(y_test, y_pred,average=\"macro\")\n",
    "        y_true_a=np.append(y_true_a,y_test)\n",
    "        y_pred_a=np.append(y_pred_a,y_pred)\n",
    "    dataa={\n",
    "        'f1_scores': scores,\n",
    "        'acc_scores': acc_scores,\n",
    "        'rec_scores': rec_scores\n",
    "    }\n",
    "    df_ = pd.DataFrame(data=dataa)\n",
    "    return df_, y_true_a, y_pred_a\n",
    "\n",
    "Xnorm_ = embeddings_norm\n",
    "y_ = dfi_keep.loc[:,\"label\"].values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "bad marshal data (unknown type code)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8918e8ac5492>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdefault_colours\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'axes.prop_cycle'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mby_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'color'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# theme colours\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Impport deeptone models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeeptone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIdentity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdata_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/Users/orestes/Documents/OTO/Projects/data/Chimps/Neural_Network_Data/sntk_3\"\u001b[0m \u001b[0;31m#\"../../../data/Chimps/Clean_selection_no_bracket\" # Define data location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'mkdir {data_folder}/slow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/deeptone/models.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;31m# Model was not loaded before\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m             self._model = tf.keras.models.load_model(\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                 custom_objects={'keras': tf.keras,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    180\u001b[0m     if (h5py is not None and (\n\u001b[1;32m    181\u001b[0m         isinstance(filepath, h5py.File) or h5py.is_hdf5(filepath))):\n\u001b[0;32m--> 182\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mhdf5_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model_from_hdf5\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36mload_model_from_hdf5\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No model found in config file.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     model = model_config_lib.model_from_config(model_config,\n\u001b[0m\u001b[1;32m    178\u001b[0m                                                custom_objects=custom_objects)\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/saving/model_config.py\u001b[0m in \u001b[0;36mmodel_from_config\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m     53\u001b[0m                     '`Sequential.from_config(config)`?')\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/layers/serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \"\"\"\n\u001b[1;32m    170\u001b[0m   \u001b[0mpopulate_deserializable_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   return generic_utils.deserialize_keras_object(\n\u001b[0m\u001b[1;32m    172\u001b[0m       \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOCAL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'custom_objects'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         return cls.from_config(\n\u001b[0m\u001b[1;32m    355\u001b[0m             \u001b[0mcls_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             custom_objects=dict(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m   2236\u001b[0m     \u001b[0;31m# be constructed for FunctionalModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2237\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2238\u001b[0;31m     return functional.Functional.from_config(\n\u001b[0m\u001b[1;32m   2239\u001b[0m         config, custom_objects=custom_objects)\n\u001b[1;32m   2240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIn\u001b[0m \u001b[0mcase\u001b[0m \u001b[0mof\u001b[0m \u001b[0mimproperly\u001b[0m \u001b[0mformatted\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \"\"\"\n\u001b[0;32m--> 616\u001b[0;31m     input_tensors, output_tensors, created_layers = reconstruct_from_config(\n\u001b[0m\u001b[1;32m    617\u001b[0m         config, custom_objects)\n\u001b[1;32m    618\u001b[0m     model = cls(inputs=input_tensors, outputs=output_tensors,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mreconstruct_from_config\u001b[0;34m(config, custom_objects, created_layers)\u001b[0m\n\u001b[1;32m   1202\u001b[0m   \u001b[0;31m# First, we create all layers and enqueue nodes to be processed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mlayer_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'layers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1204\u001b[0;31m     \u001b[0mprocess_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1205\u001b[0m   \u001b[0;31m# Then we process nodes in order of layer depth.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1206\u001b[0m   \u001b[0;31m# Nodes that cannot yet be processed (if the inbound node\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mprocess_layer\u001b[0;34m(layer_data)\u001b[0m\n\u001b[1;32m   1184\u001b[0m       \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdeserialize_layer\u001b[0m  \u001b[0;31m# pylint: disable=g-import-not-at-top\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m       \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialize_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m       \u001b[0mcreated_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/layers/serialization.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(config, custom_objects)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \"\"\"\n\u001b[1;32m    170\u001b[0m   \u001b[0mpopulate_deserializable_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m   return generic_utils.deserialize_keras_object(\n\u001b[0m\u001b[1;32m    172\u001b[0m       \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m       \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOCAL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALL_OBJECTS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'custom_objects'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marg_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m         return cls.from_config(\n\u001b[0m\u001b[1;32m    355\u001b[0m             \u001b[0mcls_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             custom_objects=dict(\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mfrom_config\u001b[0;34m(cls, config, custom_objects)\u001b[0m\n\u001b[1;32m   1003\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mfrom_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m     function = cls._parse_function_from_config(\n\u001b[0m\u001b[1;32m   1006\u001b[0m         config, custom_objects, 'function', 'module', 'function_type')\n\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36m_parse_function_from_config\u001b[0;34m(cls, config, custom_objects, func_attr_name, module_attr_name, func_type_attr_name)\u001b[0m\n\u001b[1;32m   1055\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfunction_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'lambda'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m       \u001b[0;31m# Unsafe deserialization from bytecode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m       function = generic_utils.func_load(\n\u001b[0m\u001b[1;32m   1058\u001b[0m           config[func_attr_name], globs=globs)\n\u001b[1;32m   1059\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mfunction_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'raw'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/tensorflow/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mfunc_load\u001b[0;34m(code, defaults, closure, globs)\u001b[0m\n\u001b[1;32m    455\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mUnicodeEncodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinascii\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mraw_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'raw_unicode_escape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m   \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmarshal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mglobs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0mglobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: bad marshal data (unknown type code)"
     ]
    }
   ],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "default_colours = plt.rcParams['axes.prop_cycle'].by_key()['color']  # theme colours\n",
    "# Impport deeptone models\n",
    "model = deeptone.models.Identity.get_model()\n",
    "data_folder = \"/Users/orestes/Documents/OTO/Projects/data/Chimps/Neural_Network_Data/sntk_2\" #\"../../../data/Chimps/Clean_selection_no_bracket\" # Define data location\n",
    "os.system(f'mkdir {data_folder}/slow')\n",
    "audio_files = [f for f in os.listdir(data_folder) if f.lower().endswith('.wav')]\n",
    "# slow_downs = [16000/44100]\n",
    "sliding_window_start=0\n",
    "plot_PCA = False\n",
    "save_embeddings = False\n",
    "clustering_variable = \"ID\"\n",
    "individuals = [\"Zed\",\"Squibs\",\"Nambi\"]\n",
    "quality = ['', 'd','i', '05-c', 'c'] # 'u', 'n', \n",
    "version = deeptone.__version__\n",
    "down_sample_type = \"sox\" # alternatively you can choose \"sox\"\n",
    "normalise_embeddings = False\n",
    "n_comps = 3\n",
    "expected_sr = 16000#44100\n",
    "target_down_sr = 16000\n",
    "remove_unexpected_sr = True\n",
    "if down_sample_type == \"librosa\":\n",
    "    sr = target_down_sr\n",
    "else:\n",
    "    sr = None\n",
    "cluster_type = \"spectral\" # otherwise choose \"kmeans\"\n",
    "if cluster_type == \"kmeans\":\n",
    "    cluster_options=\"k-means++\"\n",
    "elif cluster_type == \"spectral\":\n",
    "    cluster_options=\"rbf\" \n",
    "unexptected_srs= []\n",
    "for audio_file in audio_files:\n",
    "    if sox.file_info.sample_rate(os.path.join(data_folder,audio_file)) != expected_sr:\n",
    "        unexptected_srs.append(audio_file)\n",
    "if len(unexptected_srs) > 0 & remove_unexpected_sr:\n",
    "    audio_files = [i for i in audio_files if i not in remove_unexpected_sr]\n",
    "    Warning(f'Found files with unxepcted sample rates: Removing {remove_unexpected_sr}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = dict()\n",
    "dfs_norm = dict()\n",
    "df = pd.read_csv(os.path.join(data_folder, 'Chimps_db.csv'), index_col='Unnamed: 0')\n",
    "df_ = df.groupby('path_to_file').agg({'label': 'first', 'call_type': 'first', 'len': 'first'}).reset_index()\n",
    "# df_ = pd.DataFrame(data=audio_files,columns=[\"path_to_file\"])\n",
    "X_ = np.zeros((len(df_), 128))\n",
    "Xnorm_ = np.zeros_like(X_)\n",
    "for row in tqdm.tqdm(df_.iterrows()):\n",
    "    x, sr = librosa.load(os.path.join(data_folder, row[1].path_to_file), sr=None)\n",
    "    assert sr == 16000\n",
    "    identity_file = deeptone.models.Identity.predict(x)[0,:,:]\n",
    "    dfs.update({row[1].path_to_file: identity_file})  \n",
    "    identity_file_norm = deeptone.models.Identity.predict(x/max(x))[0,:,:]\n",
    "    dfs_norm.update({row[1].path_to_file: identity_file_norm}) \n",
    "    identity_squashed = identity_file.mean(axis=0)\n",
    "    identity_squashed_norm = identity_file_norm.mean(axis=0)\n",
    "    df_.loc[row[0], 'identity_squashed_str'] = identity_squashed.tostring()\n",
    "    df_.loc[row[0], 'identity_squashed_norm_str'] = identity_squashed_norm.tostring()\n",
    "    info_row = row[1].path_to_file.replace(\"(\",\"_\").replace(\")\",\"_\").replace(\".wav\",\"_.wav\").replace(\"__.wav\",\"_.wav\").split(\"_\")\n",
    "    df_.loc[row[0], \"label\"] = info_row[0]\n",
    "    df_.loc[row[0], \"call_type\"] = info_row[1]\n",
    "    df_.loc[row[0], \"quality\"] = info_row[-2]\n",
    "    X_[row[0], :] = identity_squashed\n",
    "    Xnorm_[row[0], :] = identity_squashed_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_.loc[:, 'label_idx'] = df_.label.astype('category').cat.codes\n",
    "y_ = df_.label_idx.values\n",
    "dfi_keep = df_#[dfi[clustering_variable].isin(individuals)]\n",
    "# 'd', 'u', 'n', 'i', '05-c', 'c'\n",
    "# quality = ['d', 'i', '05-c', 'c','u', 'n' ] #  'u', 'n' \n",
    "# dfi_keep = dfi_keep[dfi_keep[\"quality\"].isin(quality)]\n",
    "# dfi_keep = dfi_keep[~dfi_keep[\"call_type\"].isin(['pg'])]\n",
    "dfi_keep = dfi_keep.iloc[np.nonzero(dfi_keep.call_type.values != 'pg')[0],:]\n",
    "dfi_keep.loc[:,\"call_type\"] = [\"Combination\" if \"-\" in f else f for f in dfi_keep.call_type.values]\n",
    "plot_count_variable_combo(\"label\",\"call_type\",dfi_keep)\n",
    "embeddings, embeddings_norm = embedding_processing(dfi_keep,dfs,dfs_norm)\n",
    "#SVM\n",
    "df_norm, y_t_norm, y_p_norm = SVM(embeddings_norm,dfi_keep.loc[:,\"label\"].values,50,False,1,stochasticc=True)\n",
    "df_norm['Classifier'] = 'SVM'\n",
    "df_norm.f1_scores.values.mean()\n",
    "#RF\n",
    "df_RF, y_t_RF, y_p_RF =  RandomForrest(embeddings_norm,dfi_keep.loc[:,\"label\"].values,50,1)\n",
    "df_RF['Classifier'] = 'RF'\n",
    "df_RF.f1_scores.values.mean()\n",
    "#GP\n",
    "df_GPC, y_t_GCP, y_p_GCP =  GP(embeddings_norm,dfi_keep.loc[:,\"label\"].values,50,1)\n",
    "df_GPC['Classifier'] = 'GP'\n",
    "df_GPC.f1_scores.values.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titlee = \"Pant-grunts, Pant-hoots & Screams\"\n",
    "all_data = df_norm.loc[:,['f1_scores','acc_scores','Classifier']]\n",
    "all_data=all_data.append(df_RF.loc[:,['f1_scores','acc_scores','Classifier']])\n",
    "all_data=all_data.append(df_GPC.loc[:,['f1_scores','acc_scores','Classifier']])\n",
    "all_data = all_data.rename(columns={\"f1_scores\": \"F1\", \"acc_scores\": \"Accuracy\"})\n",
    "all_data_melt = pd.melt(all_data,id_vars=['Classifier'],value_vars=['F1','Accuracy'])\n",
    "plt.close()\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "ax = sns.boxplot(x=\"variable\", y=\"value\", hue=\"Classifier\",data=all_data_melt, palette=\"Set1\")\n",
    "ax.set(ylim=(0.55, 1),title=titlee,ylabel=\"Average score\")\n",
    "ax.legend_.remove()\n",
    "ax.figure.savefig(\"score_dist1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-1e11febabffc>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-1e11febabffc>\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    report_melt = report_melt.rename(columns='variable':'Chimp')\u001b[0m\n\u001b[0m                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "report = pd.DataFrame(classification_report(y_t_GCP, y_p_GCP,output_dict=True))\n",
    "report['Classifier'] = 'GP'\n",
    "report_RF = pd.DataFrame(classification_report(y_t_RF, y_p_RF,output_dict=True))\n",
    "report_RF['Classifier'] = 'RF'\n",
    "report_norm = pd.DataFrame(classification_report(y_t_norm, y_p_norm,output_dict=True))\n",
    "report_norm['Classifier'] = 'SVM'\n",
    "report=report.append(report_RF)#,report_norm)\n",
    "report=report.append(report_norm)\n",
    "report['metric'] = report.index\n",
    "report_melt = pd.melt(report,id_vars=['Classifier','metric'],value_vars=['Nambi','Squibs','Zed'])\n",
    "report_melt = report_melt.rename(columns={'variable':'Chimp'})\n",
    "ax = sns.catplot(kind=\"point\",x=\"Classifier\",y=\"value\", hue=\"Chimp\", palette=\"ch:.25\", \n",
    "                 data=report_melt[report_melt['metric'].isin([\"precision\"])])\n",
    "ax.set(ylim=(0.6, 1),title=\"(b) \"+titlee,xlabel=None,xticklabels=[],ylabel=None,yticklabels=[])\n",
    "ax._legend.remove()\n",
    "ax.savefig(\"Precision1.png\")\n",
    "ax = sns.catplot(kind=\"point\",x=\"Classifier\",y=\"value\", hue=\"Chimp\", palette=\"ch:.25\", \n",
    "                 data=report_melt[report_melt['metric'].isin([\"recall\"])])\n",
    "ax.set(ylim=(0.4, 1),title=\"(c)\",ylabel=\"Recall score\") #ylabel=None,yticklabels=[]\n",
    "ax._legend.remove()\n",
    "plt.legend(loc='lower left')\n",
    "ax.savefig(\"Recall.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tpot import TPOTClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings_norm,dfi_keep.loc[:,\"label\"].values, test_size = 0.25, random_state = 42)\n",
    "tpot = TPOTClassifier(generations=10, population_size=100, verbosity=2, random_state=42, scoring = 'balanced_accuracy')\n",
    "tpot.fit(X_train, y_train)\n",
    "tpot_pred = tpot.predict(X_test)\n",
    "f1_score(y_test, tpot_pred,average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"/Users/orestes/Documents/OTO/Projects/data/Chimps/Neural_Network_Data/sntk_3\" #\"../../../data/Chimps/Clean_selection_no_bracket\" # Define data location\n",
    "os.system(f'mkdir {data_folder}/slow')\n",
    "audio_files = [f for f in os.listdir(data_folder) if f.lower().endswith('.wav')]\n",
    "df = pd.read_csv(os.path.join(data_folder, 'Chimps_db.csv'), index_col='Unnamed: 0')\n",
    "df_ = df.groupby('path_to_file').agg({'label': 'first', 'call_type': 'first', 'len': 'first'}).reset_index()\n",
    "set(df_.path_to_file.values).difference(set(audio_files))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeptone",
   "language": "python",
   "name": "deeptone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
